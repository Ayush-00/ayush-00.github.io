<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ayush Gupta</title>
  
  <meta name="author" content="Ayush Gupta">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-113195086-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113195086-1');
</script>


</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ayush Gupta</name>
              </p>
              <p>
                I am a Ph.D. student at the <a href="https://aiem.jhu.edu/">AIEM lab</a>, Johns Hopkins University in the department of Computer Science. I am advised by <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Prof. Rama Chellappa</a> working on problems in Computer Vision and Machine Learning. 
                My research has two focus points - general-purpose vision language models, where I work on multimodal LLMs on a wide variety of tasks, and on fine-grained computer vision problems, where I work on person re-identification and gait recognition.
                I am supported by an IARPA grant, <a href="https://www.iarpa.gov/research-programs/briar">BRIAR</a>. 
              </p>
              
		<p>
              Previously, I obtained a B.E. in Computer Science from Birla Institute of Technology and Science (BITS), Pilani. At BITS Pilani I was working under the guidance of <a href="https://www.bits-pilani.ac.in/pilani/poonam-goyal/">Prof. Poonam Goyal</a> on generating natural language descriptions of videos, and collaborating with <a href="https://www.crcv.ucf.edu/person/rawat/">Dr. Yogesh S Rawat</a> from UCF on Gait Recognition.
               </p>

              <p style="text-align:center">
                <a href="mailto:agupt120@jhu.edu">Email</a> &nbsp/&nbsp
                <a href="data/AyushGupta.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=L-rN8kkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/AyushGupta6700">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/Ayush-00">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/AyushGupta.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/AyushGupta.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">10/2024: Our paper <a href="https://arxiv.org/abs/2311.16497v2">MimicGait: A Model-Agnostic Approach for Occluded Gait Recognition using Correlational Knowledge Distillation</a> has been accepted to WACV 2025!</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2024: Our paper <a href="https://arxiv.org/abs/2311.16497v2">GaitContour: Efficient Gait Recognition based on a Contour-Pose Representation</a> has been accepted to WACV 2025!</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2024: Starting a summer internship at <a href="https://www.sri.com/">SRI</a>! I will be working on vision-language models. </li>
		              <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2024: Our paper <a href="https://openaccess.thecvf.com/content/CVPR2024W/CLVISION/papers/Nguyen_Tackling_Domain_Shifts_in_Person_Re-Identification_A_Survey_and_Analysis_CVPRW_2024_paper.pdf">"Tackling Domain Shifts in Person Re-Identification: A Survey and Analysis"</a> has been accepted at the CVPR 2024 Continual Learning Workshop! </li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">10/2023: Our paper <a href="https://openaccess.thecvf.com/content/WACV2024/html/Gupta_You_Can_Run_but_Not_Hide_Improving_Gait_Recognition_With_WACV_2024_paper.html">You Can Run but not Hide: Improving Gait Recognition with Intrinsic Occlusion Type Awareness</a> is accepted as an oral paper at WACV 2024! See you in Hawaii!</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">08/2023: Starting a Teaching Assistantship in the Machine Perception course for Fall 2023</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">08/2022: Joined the <a href="https://aiem.jhu.edu">AIEM Lab</a> as a PhD student at Johns Hopkins University! I will be working on BRIAR, an IARPA grant.</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2021: Started collaborating with CRCV Lab at UCF for undergraduate thesis</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2020: Started working with the Language Technology Group at University of Hamburg on the CLARIN COVID Disinformation Hackathon</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2020: Started internship at Indian Institute of Remote Sensing, ISRO</li>

                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <!-- <p>
                I am interested in all things computer vision and machine learning. Specifically, my current research interests are in Multimodal LLMs, where I am working on making LLMs see the world like humans do. I also work on Gait Recognition and Person Re-ID, on vision models which can identify people from long-range, turbulent, in-the-wild data. 
              </p> -->
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vigor.png" alt="MLLMs" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision</papertitle>
              <br>
              <br>
              <strong>Ayush Gupta</strong>, <a href="https://nusci.csl.sri.com/author/anirban-roy/">Anirban Roy</a>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a>, <a href="https://www.darpa.mil/staff/dr-nathaniel-bastian">Nathaniel D. Bastian</a>, <a href="https://www.darpa.mil/staff/dr-alvaro-velasquez">Alvaro Velasquez</a>, <a href="https://susmitjha.github.io/">Susmit Jha</a>
              <br>
              <em>under submission</em>
              <p>We use multimodal LLMs for temporal grounding of question-answer pairs in unconstrained videos.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/wacv24.png" alt="YouCanRunButNotHide" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>You Can Run but not Hide: Improving Gait Recognition with Intrinsic Occlusion Type Awareness</papertitle>
              <br>
              <br>
              <strong>Ayush Gupta</strong>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a>
              <br>
              <em>WACV 2024 (Oral)</em>
              <p>We use an auxiliary occlusion detector to solve the occlusion problem in long range gait recognition.</p>
              <a href="https://ayush-00.github.io/occ-aware-website/">Project Website</a> /
              <a href="https://arxiv.org/abs/2312.02290">arXiv</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mimicgait.png" alt="MimicGait" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>MimicGait: A Model-Agnostic Approach for Occluded Gait Recognition using Correlational Knowledge Distillation</papertitle>
              <br>
              <br>
              <strong>Ayush Gupta</strong>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a>
              <br>
              <em>WACV 2025</em>
              <p>We tackle the occlusion problem in gait recognition using correlational knowledge distillation.</p>
              <a href="https://ayush-00.github.io/mimicgait-website/">Project Website</a> /
              <a href="https://arxiv.org/abs/2501.15666">arXiv</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gaitcontour.png" alt="GaitContour" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>GaitContour: Efficient Gait Recognition based on a Contour-Pose Representation</papertitle>
              <br>
              <br>
              <a href="https://www.linkedin.com/in/yuxiang-guo-a7b105163/">Yuxiang Guo</a>, <a href="https://anshulbshah.github.io">Anshul Shah</a>, <a href="https://joellliu.github.io">Jiang Liu</a>, <strong>Ayush Gupta</strong>, <a href="https://sites.google.com/view/cheng-peng/home">Cheng Peng</a>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a>
              <br>
              <em>WACV 2025</em>
              <p>We develop a novel, efficient contour-based representation for gait recognition.</p>
              <a href="https://arxiv.org/abs/2311.16497#">arXiv</a> 
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/frailty.png" alt="Frailty" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Transfer Learning for Frailty Classification in Older Adults</papertitle>
              
              <br>
              <br>
                Laura McDaniel, <strong>Ayush Gupta*</strong>, Ime Essien, Ryan Roemmich, <a href="https://profiles.hopkinsmedicine.org/provider/peter-abadir/2702609">Peter Abadir</a>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a>
              <br>
              <em>under submission</em>
              <p>Using computer vision techniques to diagnose frailty among older adults.</p>
              <!-- <a href="data/Video_Captioning.pdf">Paper</a> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/echosam.png" alt="MVA" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>EchoSAM: Predicting Ejection Fraction using Segmentation Guided Vision Transformers</papertitle></a>
              <br>
              <br>
              <a href="https://aiem.jhu.edu/people/basudha-pal/">Basudha Pal</a>, <strong>Ayush Gupta</strong> ,<a href="https://engineering.jhu.edu/faculty/vishal-patel/">Vishal Patel</a>
              <br>
              <p>Predicting the Ejection Fraction from ultrasound images of the heart, utilizing the Segment Anything Model.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/domain-shift-reid.png" alt="DomainShiftReID" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Tackling Domain Shifts in Person Re-Identification: A Survey and Analysis</papertitle>
              <br>
              <br>
              <a href="https://www.linkedin.com/in/vuongdnguyen/">Vuong Nguyen</a>, Samiha Mirza, Abdollah Zakeri, <strong>Ayush Gupta</strong>, Rahma Aloui, Khadija Khaldi, Pranav Mantini, Shishir Shah, Fatima Merchant 
              <br>
              <em>CVPR 2024 Continual Learning Workshop</em>
              <p>A survey on domain shift in Person Re-ID.</p>
              <a href="https://openaccess.thecvf.com/content/CVPR2024W/CLVISION/papers/Nguyen_Tackling_Domain_Shifts_in_Person_Re-Identification_A_Survey_and_Analysis_CVPRW_2024_paper.pdf">Paper</a>  
            </td>
          </tr>
          


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gaitzero-tsm.png" alt="GaitZero" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
                <papertitle>GaitZero: Temporal Self-similarity for Unsupervised Gait Recognition</papertitle>
              <br>
              <br>
              <strong>Ayush Gupta</strong>, Alexander Matasa, Shruti Vyas, <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>
              <br>
              <p>Developing a novel technique for unsupervised gait recognition using temporal self similarity.</p>
              <!-- <a href="data/GaitZero.pdf">Paper</a> -->
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/video_captioning.png" alt="AttentionVehicle" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Visually Guided Knowledge selection for Video Captioning</papertitle>
              
              <br>
              <br>
              <strong>Ayush Gupta*</strong>, Ashrya Agrawal*, Poonam Goyal, Navneet Goyal
              <br>
              <p>An approach for generating natural language captions of videos using external knowledge bases.</p>
              <a href="data/Video_Captioning.pdf">Paper</a>
            </td>
          </tr>

          


        </tbody></table>

				
      
					
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                Template Credits : <a href="https://jonbarron.info/">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
